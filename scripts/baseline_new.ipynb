{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5426e2bb-2216-494c-85b5-79547a28dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb1d3f5-f7c8-40b9-bf9c-ec13bc258bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed MIMIC-III data...\n",
      "Loaded 52726 records\n",
      "Creating train and test splits...\n",
      "Created splits: 42180 train, 10546 test\n",
      "Processing ICD codes...\n",
      "Total unique ICD-9 codes: 6918\n",
      "Preparing data...\n",
      "Creating TF-IDF features...\n",
      "TF-IDF features shape: (42180, 10000)\n",
      "TF-IDF processing time: 92.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# MIMIC-III Baseline Model Training: TF-IDF + Logistic Regression with Cross Validation\n",
    "\n",
    "\n",
    "# 1. Load the preprocessed data\n",
    "print(\"Loading preprocessed MIMIC-III data...\")\n",
    "data_path = '../data/'\n",
    "processed_data_file = os.path.join(data_path, 'mimic3_data.pkl')\n",
    "data = pd.read_pickle(processed_data_file)\n",
    "\n",
    "print(f\"Loaded {len(data)} records\")\n",
    "\n",
    "# 2. Create train and test sets (80% train, 20% test)\n",
    "print(\"Creating train and test splits...\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f\"Created splits: {len(train_data)} train, {len(test_data)} test\")\n",
    "\n",
    "# 3. Process ICD codes - build a set of all unique codes\n",
    "# Modified code to handle mixed data types in ICD codes\n",
    "print(\"Processing ICD codes...\")\n",
    "all_codes = set()\n",
    "for codes in data['ICD9_CODE']:\n",
    "    # Convert any non-string codes to strings and filter out NaN values\n",
    "    valid_codes = [str(code) for code in codes if pd.notna(code)]\n",
    "    all_codes.update(valid_codes)\n",
    "\n",
    "# Now all codes should be strings, so sorting will work\n",
    "code_to_idx = {code: i for i, code in enumerate(sorted(all_codes))}\n",
    "idx_to_code = {i: code for code, i in code_to_idx.items()}\n",
    "\n",
    "num_codes = len(code_to_idx)\n",
    "print(f\"Total unique ICD-9 codes: {num_codes}\")\n",
    "\n",
    "\n",
    "# 4. Create features and labels\n",
    "def prepare_data(dataset):\n",
    "    texts = dataset['TEXT'].tolist()\n",
    "    \n",
    "    # Convert ICD codes to multi-hot encoded vectors\n",
    "    labels = np.zeros((len(dataset), num_codes), dtype=np.int8)\n",
    "    for i, codes in enumerate(dataset['ICD9_CODE']):\n",
    "        for code in codes:\n",
    "            if code in code_to_idx:  # Just in case there's an unknown code\n",
    "                labels[i, code_to_idx[code]] = 1\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "train_texts, train_labels = prepare_data(train_data)\n",
    "test_texts, test_labels = prepare_data(test_data)\n",
    "\n",
    "# 5. Create TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "start_time = time.time()\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features to reduce dimensionality\n",
    "    min_df=5,            # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.5,          # Ignore terms that appear in more than 50% of documents\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    stop_words='english' # Remove English stop words\n",
    ")\n",
    "\n",
    "train_features = tfidf.fit_transform(train_texts)\n",
    "test_features = tfidf.transform(test_texts)\n",
    "\n",
    "print(f\"TF-IDF features shape: {train_features.shape}\")\n",
    "print(f\"TF-IDF processing time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f312a1-4d55-448f-b100-f8e718f27b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up cross-validation...\n",
      "Performing cross-validation...\n",
      "\n",
      "Training fold 1/5...\n",
      "  Training classifier for code 71/6918\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training classifier for code 73/6918\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training classifier for code 79/6918\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 77\u001b[0m\n\u001b[1;32m     67\u001b[0m estimator \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[1;32m     68\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     69\u001b[0m     solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     estimators\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[1;32m     79\u001b[0m     trained_indices\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 6. Cross-validation and model training\n",
    "print(\"Setting up cross-validation...\")\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {\n",
    "    'fold': [],\n",
    "    'micro_f1': [],\n",
    "    'macro_f1': [],\n",
    "    'micro_precision': [],\n",
    "    'macro_precision': [],\n",
    "    'micro_recall': [],\n",
    "    'macro_recall': []\n",
    "}\n",
    "\n",
    "def evaluate(model, features, labels):\n",
    "    # Get predictions\n",
    "    pred_probs = model.predict_proba(features)\n",
    "    \n",
    "    # Apply threshold of 0.5 for binary classification\n",
    "    preds = np.zeros_like(labels)\n",
    "    for i in range(num_codes):\n",
    "        # Check if this classifier exists (it may not if there was only one class for this code)\n",
    "        if i < len(pred_probs) and len(pred_probs[i]) > 0:\n",
    "            preds[:, i] = (pred_probs[i][:, 1] >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate micro and macro F1 scores\n",
    "    micro_f1 = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate micro and macro precision and recall\n",
    "    micro_precision = precision_score(labels, preds, average='micro', zero_division=0)\n",
    "    macro_precision = precision_score(labels, preds, average='macro', zero_division=0)\n",
    "    micro_recall = recall_score(labels, preds, average='micro', zero_division=0)\n",
    "    macro_recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_precision': micro_precision,\n",
    "        'macro_precision': macro_precision,\n",
    "        'micro_recall': micro_recall,\n",
    "        'macro_recall': macro_recall\n",
    "    }\n",
    "\n",
    "print(\"Performing cross-validation...\")\n",
    "fold = 1\n",
    "best_model = None\n",
    "best_micro_f1 = 0\n",
    "\n",
    "for train_idx, val_idx in kf.split(train_features):\n",
    "    print(f\"\\nTraining fold {fold}/{n_folds}...\")\n",
    "    \n",
    "    # Get fold data\n",
    "    X_train_fold, X_val_fold = train_features[train_idx], train_features[val_idx]\n",
    "    y_train_fold, y_val_fold = train_labels[train_idx], train_labels[val_idx]\n",
    "    \n",
    "    # Create a list to store estimators\n",
    "    estimators = []\n",
    "    trained_indices = []\n",
    "    \n",
    "    # Train a separate model for each ICD code that has both positive and negative examples\n",
    "    for i in range(num_codes):\n",
    "        # Check if there are both positive and negative examples for this code\n",
    "        if np.sum(y_train_fold[:, i] == 0) > 0 and np.sum(y_train_fold[:, i] == 1) > 0:\n",
    "            print(f\"  Training classifier for code {i+1}/{num_codes}\", end='\\r')\n",
    "            estimator = LogisticRegression(\n",
    "                C=1.0,\n",
    "                solver='saga',\n",
    "                penalty='l1',\n",
    "                max_iter=500,\n",
    "                n_jobs=-1,  # Set to 1 for individual estimator\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "            try:\n",
    "                estimator.fit(X_train_fold, y_train_fold[:, i])\n",
    "                estimators.append(estimator)\n",
    "                trained_indices.append(i)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error training classifier for code {i}: {e}\")\n",
    "        else:\n",
    "            # Skip codes without both positive and negative examples\n",
    "            pass\n",
    "    \n",
    "    print(f\"  Trained {len(estimators)}/{num_codes} classifiers\")\n",
    "    \n",
    "    # Create a model object to hold the estimators\n",
    "    model = MultiOutputClassifier(LogisticRegression())\n",
    "    model.estimators_ = estimators\n",
    "    model.classes_ = [np.array([0, 1]) for _ in estimators]\n",
    "    model.trained_indices = trained_indices  # Store which indices were trained\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate(model, X_val_fold, y_val_fold)\n",
    "    \n",
    "    print(f\"Fold {fold} validation results:\")\n",
    "    print(f\"  Micro F1: {metrics['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    cv_results['fold'].append(fold)\n",
    "    for key, value in metrics.items():\n",
    "        cv_results[key].append(value)\n",
    "    \n",
    "    # Keep track of best model\n",
    "    if metrics['micro_f1'] > best_micro_f1:\n",
    "        best_micro_f1 = metrics['micro_f1']\n",
    "        best_model = model\n",
    "        print(f\"  New best model found with micro F1: {best_micro_f1:.4f}\")\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba9cdc-efad-4187-95cb-3dbdd9b09ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing cross-validation...\n",
      "\n",
      "Training fold 1/5...\n",
      "  Training classifiers in parallel using all 32 CPUs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Modified approach to handle single-class data while utilizing all 32 CPUs\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "\n",
    "print(\"Performing cross-validation...\")\n",
    "fold = 1\n",
    "best_model = None\n",
    "best_micro_f1 = 0\n",
    "\n",
    "for train_idx, val_idx in kf.split(train_features):\n",
    "    print(f\"\\nTraining fold {fold}/{n_folds}...\")\n",
    "    \n",
    "    # Get fold data\n",
    "    X_train_fold, X_val_fold = train_features[train_idx], train_features[val_idx]\n",
    "    y_train_fold, y_val_fold = train_labels[train_idx], train_labels[val_idx]\n",
    "    \n",
    "    # Function to train a single classifier\n",
    "    def train_single_model(i):\n",
    "        # Check if there are both positive and negative examples for this code\n",
    "        if np.sum(y_train_fold[:, i] == 0) > 0 and np.sum(y_train_fold[:, i] == 1) > 0:\n",
    "            estimator = LogisticRegression(\n",
    "                C=1.0,\n",
    "                solver='saga',\n",
    "                penalty='l1',\n",
    "                max_iter=500,\n",
    "                verbose=0,\n",
    "                random_state=42\n",
    "            )\n",
    "            try:\n",
    "                estimator.fit(X_train_fold, y_train_fold[:, i])\n",
    "                return (i, estimator)\n",
    "            except Exception as e:\n",
    "                print(f\"  Error training classifier for code {i}: {e}\")\n",
    "                return (i, None)\n",
    "        else:\n",
    "            # For codes with only one class, create a dummy classifier that always predicts that class\n",
    "            majority_class = int(np.bincount(y_train_fold[:, i]).argmax())\n",
    "            return (i, DummyClassifier(strategy='constant', constant=majority_class))\n",
    "    \n",
    "    # Run training in parallel across all 32 cores\n",
    "    print(\"  Training classifiers in parallel using all 32 CPUs...\")\n",
    "    results = Parallel(n_jobs=32)(delayed(train_single_model)(i) for i in range(num_codes))\n",
    "    \n",
    "    # Process results\n",
    "    estimators = []\n",
    "    trained_indices = []\n",
    "    for i, estimator in results:\n",
    "        if estimator is not None:\n",
    "            estimators.append(estimator)\n",
    "            trained_indices.append(i)\n",
    "    \n",
    "    print(f\"  Trained {len(estimators)}/{num_codes} classifiers\")\n",
    "    \n",
    "    # Create a model object to hold the estimators\n",
    "    model = MultiOutputClassifier(LogisticRegression())\n",
    "    model.estimators_ = estimators\n",
    "    model.classes_ = [np.array([0, 1]) for _ in estimators]\n",
    "    model.trained_indices = trained_indices  # Store which indices were trained\n",
    "    \n",
    "    # Modify the predict_proba function to handle our custom structure\n",
    "    def custom_predict_proba(self, X):\n",
    "        probas = []\n",
    "        for i, estimator in enumerate(self.estimators_):\n",
    "            # For regular LogisticRegression models\n",
    "            if isinstance(estimator, LogisticRegression):\n",
    "                probas.append(estimator.predict_proba(X))\n",
    "            # For DummyClassifier models\n",
    "            elif isinstance(estimator, DummyClassifier):\n",
    "                constant_class = estimator.constant_\n",
    "                n_samples = X.shape[0]\n",
    "                if constant_class == 0:\n",
    "                    # Always predict class 0\n",
    "                    proba = np.zeros((n_samples, 2))\n",
    "                    proba[:, 0] = 1.0\n",
    "                else:\n",
    "                    # Always predict class 1\n",
    "                    proba = np.zeros((n_samples, 2))\n",
    "                    proba[:, 1] = 1.0\n",
    "                probas.append(proba)\n",
    "        return probas\n",
    "    \n",
    "    # Attach the custom method to the model\n",
    "    import types\n",
    "    model.predict_proba = types.MethodType(custom_predict_proba, model)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate(model, X_val_fold, y_val_fold)\n",
    "    \n",
    "    print(f\"Fold {fold} validation results:\")\n",
    "    print(f\"  Micro F1: {metrics['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    cv_results['fold'].append(fold)\n",
    "    for key, value in metrics.items():\n",
    "        cv_results[key].append(value)\n",
    "    \n",
    "    # Keep track of best model\n",
    "    if metrics['micro_f1'] > best_micro_f1:\n",
    "        best_micro_f1 = metrics['micro_f1']\n",
    "        best_model = model\n",
    "        print(f\"  New best model found with micro F1: {best_micro_f1:.4f}\")\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994424f-3efb-42f9-bc8d-921a3e1986b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Final model training and evaluation\n",
    "print(\"\\nTraining final model on all training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a list to store estimators\n",
    "final_estimators = []\n",
    "trained_indices = []\n",
    "\n",
    "# Train a separate model for each ICD code that has both positive and negative examples\n",
    "for i in range(num_codes):\n",
    "    # Check if there are both positive and negative examples for this code\n",
    "    if np.sum(train_labels[:, i] == 0) > 0 and np.sum(train_labels[:, i] == 1) > 0:\n",
    "        print(f\"Training classifier for code {i+1}/{num_codes}\", end='\\r')\n",
    "        estimator = LogisticRegression(\n",
    "            C=1.0,\n",
    "            solver='saga',\n",
    "            penalty='l1',\n",
    "            max_iter=100,\n",
    "            n_jobs=1,  # Set to 1 for individual estimator\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        try:\n",
    "            estimator.fit(train_features, train_labels[:, i])\n",
    "            final_estimators.append(estimator)\n",
    "            trained_indices.append(i)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training classifier for code {i}: {e}\")\n",
    "    else:\n",
    "        # Skip codes without both positive and negative examples\n",
    "        pass\n",
    "\n",
    "print(f\"Trained {len(final_estimators)}/{num_codes} classifiers\")\n",
    "\n",
    "# Create a model object to hold the estimators\n",
    "final_model = MultiOutputClassifier(LogisticRegression())\n",
    "final_model.estimators_ = final_estimators\n",
    "final_model.classes_ = [np.array([0, 1]) for _ in final_estimators]\n",
    "final_model.trained_indices = trained_indices  # Store which indices were trained\n",
    "\n",
    "print(f\"Final model training time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa332acc-94cd-48fe-a7b3-6128232b5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate on training set\n",
    "train_metrics = evaluate(final_model, train_features, train_labels)\n",
    "print(\"\\nTraining set results:\")\n",
    "for key, value in train_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate(final_model, test_features, test_labels)\n",
    "print(\"\\nTest set results:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# 8. Analyze ICD code distribution and performance\n",
    "print(\"\\nAnalyzing ICD code distribution...\")\n",
    "\n",
    "# Count occurrences of each ICD code\n",
    "code_counts = np.sum(train_labels, axis=0)\n",
    "# Sort ICD codes by frequency\n",
    "sorted_idx = np.argsort(-code_counts)\n",
    "top_codes = [idx_to_code[i] for i in sorted_idx[:20]]\n",
    "top_counts = code_counts[sorted_idx[:20]]\n",
    "\n",
    "# Plot top 20 most frequent ICD codes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(20), top_counts)\n",
    "plt.xticks(range(20), top_codes, rotation=90)\n",
    "plt.title('Top 20 Most Frequent ICD-9 Codes in Training Data')\n",
    "plt.xlabel('ICD-9 Code')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_path, 'icd_distribution.png'))\n",
    "plt.show()\n",
    "\n",
    "# 9. Plot cross-validation performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cv_results['fold'], cv_results['micro_f1'], 'o-', label='Micro F1')\n",
    "plt.plot(cv_results['fold'], cv_results['macro_f1'], 'o-', label='Macro F1')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Cross-Validation Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(data_path, 'cv_performance.png'))\n",
    "plt.show()\n",
    "\n",
    "# 10. Save the model and related artifacts\n",
    "print(\"Saving model and artifacts...\")\n",
    "output_dir = os.path.join(data_path, 'baseline_model')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open(os.path.join(output_dir, 'tfidf_vectorizer.pkl'), 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# Save the trained final model\n",
    "with open(os.path.join(output_dir, 'lr_model.pkl'), 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# Save the code mappings\n",
    "with open(os.path.join(output_dir, 'code_mappings.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'code_to_idx': code_to_idx,\n",
    "        'idx_to_code': idx_to_code\n",
    "    }, f)\n",
    "\n",
    "# Save the CV results\n",
    "cv_df.to_csv(os.path.join(output_dir, 'cv_results.csv'), index=False)\n",
    "\n",
    "# Save the evaluation metrics\n",
    "metrics = {\n",
    "    'train': train_metrics,\n",
    "    'test': test_metrics,\n",
    "    'cv': cv_df.to_dict()\n",
    "}\n",
    "with open(os.path.join(output_dir, 'metrics.pkl'), 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "print(\"\\nBaseline model training and evaluation complete!\")\n",
    "print(f\"Model and artifacts saved to {output_dir}\")\n",
    "\n",
    "# 11. Display example predictions\n",
    "print(\"\\nExample predictions for 5 random test samples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(test_texts)), 5)\n",
    "\n",
    "# In the display example predictions section, modify the code:\n",
    "for idx in sample_indices:\n",
    "    # Get true labels\n",
    "    true_codes = [idx_to_code[i] for i in np.where(test_labels[idx] == 1)[0]]\n",
    "    \n",
    "    # Get predicted probabilities for trained classifiers only\n",
    "    pred_probs = np.zeros(num_codes)\n",
    "    for i, estimator_idx in enumerate(final_model.trained_indices):\n",
    "        pred_probs[estimator_idx] = final_model.estimators_[i].predict_proba(test_features[idx])[0][1]\n",
    "    \n",
    "    # Get top 5 predicted codes with highest probability\n",
    "    top_pred_indices = np.argsort(-pred_probs)[:5]\n",
    "    pred_codes = [(idx_to_code[i], pred_probs[i]) for i in top_pred_indices if pred_probs[i] > 0]\n",
    "    \n",
    "    print(f\"\\nSample {idx}:\")\n",
    "    print(f\"Text snippet: {test_texts[idx][:100]}...\")\n",
    "    print(f\"True codes ({len(true_codes)}):\")\n",
    "    for code in true_codes[:5]:\n",
    "        print(f\"  - {code}\")\n",
    "    if len(true_codes) > 5:\n",
    "        print(f\"  - ... and {len(true_codes) - 5} more\")\n",
    "    \n",
    "    print(f\"Top 5 predicted codes (with probability):\")\n",
    "    for code, prob in pred_codes:\n",
    "        print(f\"  - {code}: {prob:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
