{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c515a-b2ff-4cd3-925d-d5f49484c01a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from strategies import *\n",
    "import random\n",
    "from collections import Counter\n",
    "from Classifier import TFIDF_LR_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf136da-6729-466e-9eeb-c0ceb9da264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for active learning main loop\n",
    "def active_learning_loop(data, data_size, initial_size, batch_size, n_iterations, method='diversity sampling', n_committee=None, p_min=0.1):\n",
    "    # === Step 1: Load data ===\n",
    "    # Filter out data to the required size\n",
    "    data = data\n",
    "\n",
    "    # === Step 2: Select top-N labels (e.g., top 100) ===\n",
    "    all_labels = [code for label_list in data['ICD9_CODE'] for code in label_list]\n",
    "    top_labels = set([label for label, _ in Counter(all_labels).most_common(100)])\n",
    "\n",
    "    data['filtered_labels'] = data['ICD9_CODE'].apply(lambda codes: [c for c in codes if c in top_labels])\n",
    "    data = data[data['filtered_labels'].map(len) > 0]  # Remove samples with no labels\n",
    "\n",
    "    # === Step 3: Construct training and validation sets ===\n",
    "    texts = data['TEXT'].tolist()\n",
    "    labels = data['filtered_labels'].tolist()\n",
    "    X_pool, X_val, y_pool, y_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Optional: Limit dataset size for debugging\n",
    "    X_pool = X_pool[:data_size]\n",
    "    y_pool = y_pool[:data_size]\n",
    "\n",
    "    # === Step 4: Initialize labeled / unlabeled pool ===\n",
    "    X_labeled = X_pool[:initial_size]\n",
    "    y_labeled = y_pool[:initial_size]\n",
    "    X_unlabeled = X_pool[initial_size:]\n",
    "    y_unlabeled = y_pool[initial_size:]\n",
    "\n",
    "    # === Step 5: Initialize evaluation components ===\n",
    "    micro_f1_list = []\n",
    "    macro_f1_list = []\n",
    "\n",
    "    # === Step 6: Active Learning main loop ===\n",
    "    for i in range(n_iterations):\n",
    "        print(f\"Iter {i+1}/{n_iterations} | Labeled size: {len(X_labeled)}\")\n",
    "\n",
    "        # Train TF-IDF + LR model\n",
    "        clf = TFIDF_LR_Classifier()\n",
    "        clf.fit(X_labeled, y_labeled)\n",
    "\n",
    "        # Validate model on validation set\n",
    "        eval_metrics = clf.evaluate(X_val, y_val)\n",
    "        micro_f1_list.append(eval_metrics['micro_f1'])\n",
    "        macro_f1_list.append(eval_metrics['macro_f1'])\n",
    "        print(f\"Micro-F1: {eval_metrics['micro_f1']:.4f} | Macro-F1: {eval_metrics['macro_f1']:.4f}\\n\")\n",
    "\n",
    "        \n",
    "        selected_idx = []\n",
    "\n",
    "        if method == 'diversity sampling': \n",
    "            # === Diversity Sampling (KMeans) ===\n",
    "            embeddings = clf.vectorizer.transform(X_unlabeled).toarray()\n",
    "            selected_idx = diversity_sampling_kmeans(\n",
    "                embeddings=X_unlabeled_vec,\n",
    "                batch_size=batch_size,\n",
    "                n_clusters=batch_size,\n",
    "                mode='centroid'  # You can replace this with 'border' or 'random'\n",
    "            )\n",
    "            \n",
    "        elif method == 'uncertainty sampling':\n",
    "            # === Uncertainty Sampling ===\n",
    "            probs = clf.predict_proba(X_unlabeled)\n",
    "            selected_idx = uncertainty_sampling(\n",
    "                probabilities=probs, \n",
    "                batch_size=batch_size, \n",
    "                mode='least_confidence') # You can replace this with 'binary_entropy' or 'smallest_margin'\n",
    "            \n",
    "        elif method == 'query by committee':\n",
    "            # === QBC: train multiple committee models on different subsets ===\n",
    "            if n_committee is None:\n",
    "                n_committee = 3\n",
    "            comm_probs_list = []\n",
    "            for seed in range(n_committee):\n",
    "                indices = np.random.choice(len(X_labeled), size=len(X_labeled), replace=True)\n",
    "                X_sample = [X_labeled[i] for i in indices]\n",
    "                y_sample = [y_labeled[i] for i in indices]\n",
    "                member = TFIDF_LR_Classifier()\n",
    "                member.fit(X_sample, y_sample)\n",
    "                comm_probs_list.append(member.predict_proba(X_unlabeled))\n",
    "\n",
    "            # === QBC ===\n",
    "            selected_idx = query_by_committee(\n",
    "                comm_probs_list=comm_probs_list,\n",
    "                batch_size=batch_size,\n",
    "                mode='vote_entropy'  # or 'kl_divergence'\n",
    "            )\n",
    "\n",
    "        elif method == 'expected model change':\n",
    "            # === Expected Model Change ===\n",
    "            X_unlabeled_vec = clf.vectorizer.transform(X_unlabeled)\n",
    "            selected_idx = expected_model_change(\n",
    "                clf=clf.model,\n",
    "                X_unlabeled_vec=X_unlabeled_vec,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        elif method == 'iwal':\n",
    "            # === IWAL ===\n",
    "            '''May not successfully run since base learner LR is very slow in this AL strategy. \n",
    "            SGDClassifier is better.'''\n",
    "            X_unlabeled_vec = clf.vectorizer.transform(X_unlabeled)\n",
    "            probs = clf.predict_proba(X_unlabeled)\n",
    "            selected_idx, _ = iwal_sampling(clf.model, X_unlabeled_vec, batch_size=batch_size, p_min=p_min)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n",
    "        # Add selected samples to labeled set\n",
    "        for idx in sorted(selected_idx, reverse=True):\n",
    "            X_labeled.append(X_unlabeled.pop(idx))\n",
    "            y_labeled.append(y_unlabeled.pop(idx))\n",
    "\n",
    "    return micro_f1_list, macro_f1_list\n",
    "\n",
    "# Example usage:\n",
    "# data = pd.read_pickle('path_to_data.pkl')\n",
    "# micro_f1_list, macro_f1_list = active_learning_loop(data, data_size=5000, initial_size=300, batch_size=10, n_iterations=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
